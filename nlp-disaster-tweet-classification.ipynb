{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NLP Tutorial\n\nNLP - or *Natural Language Processing* - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.\n\nIn this tutorial we'll look at this competition's dataset, use a simple technique to process it, build a machine learning model, and submit predictions for a score!","metadata":{}},{"cell_type":"code","source":"#! pip install tensorflow --upgrade\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport re\nimport keras_tuner as kt\nimport tensorflow as tf\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom keras.backend import clear_session\nfrom keras.datasets import mnist\nfrom keras.layers import Conv2D\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import Flatten\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.metrics import MeanAbsoluteError\nfrom keras.losses import BinaryCrossentropy\nfrom keras.losses import MeanAbsoluteError\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\nnltk.download('omw-1.4')\nprint(tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-09T02:12:25.794163Z","iopub.execute_input":"2022-10-09T02:12:25.794885Z","iopub.status.idle":"2022-10-09T02:12:28.355106Z","shell.execute_reply.started":"2022-10-09T02:12:25.794800Z","shell.execute_reply":"2022-10-09T02:12:28.353996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-10-09T02:12:28.356918Z","iopub.execute_input":"2022-10-09T02:12:28.357925Z","iopub.status.idle":"2022-10-09T02:12:28.396244Z","shell.execute_reply.started":"2022-10-09T02:12:28.357885Z","shell.execute_reply":"2022-10-09T02:12:28.395381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A quick look at our data\n\nLet's look at our data... first, an example of what is NOT a disaster tweet.","metadata":{}},{"cell_type":"code","source":"print(len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.398387Z","iopub.execute_input":"2022-10-09T02:12:28.399078Z","iopub.status.idle":"2022-10-09T02:12:28.405314Z","shell.execute_reply.started":"2022-10-09T02:12:28.399041Z","shell.execute_reply":"2022-10-09T02:12:28.404194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.408876Z","iopub.execute_input":"2022-10-09T02:12:28.409291Z","iopub.status.idle":"2022-10-09T02:12:28.420542Z","shell.execute_reply.started":"2022-10-09T02:12:28.409256Z","shell.execute_reply":"2022-10-09T02:12:28.419420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.422154Z","iopub.execute_input":"2022-10-09T02:12:28.422575Z","iopub.status.idle":"2022-10-09T02:12:28.432552Z","shell.execute_reply.started":"2022-10-09T02:12:28.422540Z","shell.execute_reply":"2022-10-09T02:12:28.431320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And one that is:","metadata":{}},{"cell_type":"code","source":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.434110Z","iopub.execute_input":"2022-10-09T02:12:28.434466Z","iopub.status.idle":"2022-10-09T02:12:28.442142Z","shell.execute_reply.started":"2022-10-09T02:12:28.434429Z","shell.execute_reply":"2022-10-09T02:12:28.441124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words=stopwords.words('english')\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.443690Z","iopub.execute_input":"2022-10-09T02:12:28.444091Z","iopub.status.idle":"2022-10-09T02:12:28.452591Z","shell.execute_reply.started":"2022-10-09T02:12:28.444058Z","shell.execute_reply":"2022-10-09T02:12:28.451453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_txt(text):\n    text=text.lower()\n    text=re.sub(\"[^A-Za-z0-9]\",\" \",text)\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.454337Z","iopub.execute_input":"2022-10-09T02:12:28.454713Z","iopub.status.idle":"2022-10-09T02:12:28.461374Z","shell.execute_reply.started":"2022-10-09T02:12:28.454640Z","shell.execute_reply":"2022-10-09T02:12:28.460303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_txt(text):\n    txt=clean_txt(text)\n    tokens=word_tokenize(txt)\n    # remove stopwords and lemma\n    lemmatizer = WordNetLemmatizer()\n    \n    filters=[lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n    return \" \".join(filters)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.463204Z","iopub.execute_input":"2022-10-09T02:12:28.463536Z","iopub.status.idle":"2022-10-09T02:12:28.471259Z","shell.execute_reply.started":"2022-10-09T02:12:28.463503Z","shell.execute_reply":"2022-10-09T02:12:28.470403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"clean_text\"]=train_df.text.apply(make_txt)\ntest_df[\"clean_text\"]=test_df.text.apply(make_txt)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:28.472950Z","iopub.execute_input":"2022-10-09T02:12:28.473349Z","iopub.status.idle":"2022-10-09T02:12:32.366236Z","shell.execute_reply.started":"2022-10-09T02:12:28.473316Z","shell.execute_reply":"2022-10-09T02:12:32.365191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building vectors\n\nThe theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","metadata":{}},{"cell_type":"code","source":"tokenizer=Tokenizer(num_words=500)\ntokenizer.fit_on_texts(train_df.clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.367900Z","iopub.execute_input":"2022-10-09T02:12:32.368271Z","iopub.status.idle":"2022-10-09T02:12:32.513015Z","shell.execute_reply.started":"2022-10-09T02:12:32.368233Z","shell.execute_reply":"2022-10-09T02:12:32.511989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get our training data word index\nvocab=len(tokenizer.word_index)\nvocab","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.514864Z","iopub.execute_input":"2022-10-09T02:12:32.515547Z","iopub.status.idle":"2022-10-09T02:12:32.522612Z","shell.execute_reply.started":"2022-10-09T02:12:32.515511Z","shell.execute_reply":"2022-10-09T02:12:32.521480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sequences=tokenizer.texts_to_sequences(train_df.clean_text)\ndf_test_seq=tokenizer.texts_to_sequences(test_df.clean_text)\ndf_sequences[:5]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.528848Z","iopub.execute_input":"2022-10-09T02:12:32.529117Z","iopub.status.idle":"2022-10-09T02:12:32.673744Z","shell.execute_reply.started":"2022-10-09T02:12:32.529092Z","shell.execute_reply":"2022-10-09T02:12:32.672727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len=max([len(i) for i in df_sequences])\nmax_len","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.674926Z","iopub.execute_input":"2022-10-09T02:12:32.675264Z","iopub.status.idle":"2022-10-09T02:12:32.684947Z","shell.execute_reply.started":"2022-10-09T02:12:32.675230Z","shell.execute_reply":"2022-10-09T02:12:32.683883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_padded=pad_sequences(df_sequences,maxlen=max_len)\ndf_test_padded=pad_sequences(df_test_seq,maxlen=max_len)\ndf_padded","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.686482Z","iopub.execute_input":"2022-10-09T02:12:32.686975Z","iopub.status.idle":"2022-10-09T02:12:32.751049Z","shell.execute_reply.started":"2022-10-09T02:12:32.686940Z","shell.execute_reply":"2022-10-09T02:12:32.750115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=np.array(df_padded)\ny=train_df.target.values","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.752452Z","iopub.execute_input":"2022-10-09T02:12:32.752815Z","iopub.status.idle":"2022-10-09T02:12:32.758940Z","shell.execute_reply.started":"2022-10-09T02:12:32.752782Z","shell.execute_reply":"2022-10-09T02:12:32.757466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors_dirty = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\nexample_train_vectors_clean = count_vectorizer.fit_transform(train_df[\"clean_text\"][0:5])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.760540Z","iopub.execute_input":"2022-10-09T02:12:32.761082Z","iopub.status.idle":"2022-10-09T02:12:32.773200Z","shell.execute_reply.started":"2022-10-09T02:12:32.761049Z","shell.execute_reply":"2022-10-09T02:12:32.772287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(\"DIRTY\")\nprint(example_train_vectors_dirty[0].todense().shape)\nprint(example_train_vectors_dirty[0].todense())\nprint(\"CLEAN\")\nprint(example_train_vectors_clean[0].todense().shape)\nprint(example_train_vectors_clean[0].todense())","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.774618Z","iopub.execute_input":"2022-10-09T02:12:32.775184Z","iopub.status.idle":"2022-10-09T02:12:32.784932Z","shell.execute_reply.started":"2022-10-09T02:12:32.775149Z","shell.execute_reply":"2022-10-09T02:12:32.783876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above tells us that:\n1. There are 54 unique words (or \"tokens\") in the first five tweets.\n2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n\nNow let's create vectors for all of our tweets.","metadata":{}},{"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_df[\"clean_text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"clean_text\"])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.786523Z","iopub.execute_input":"2022-10-09T02:12:32.786953Z","iopub.status.idle":"2022-10-09T02:12:32.935382Z","shell.execute_reply.started":"2022-10-09T02:12:32.786919Z","shell.execute_reply":"2022-10-09T02:12:32.934472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Our model\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","metadata":{}},{"cell_type":"code","source":"len(train_vectors.todense())\n\nlen(np.array(train_vectors.todense()[0])[0])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:32.936766Z","iopub.execute_input":"2022-10-09T02:12:32.937107Z","iopub.status.idle":"2022-10-09T02:12:33.307000Z","shell.execute_reply.started":"2022-10-09T02:12:32.937073Z","shell.execute_reply":"2022-10-09T02:12:33.305851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model_1(hp):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(input_dim=len(np.array(train_vectors.todense()[0])[0]), output_dim=64, input_length=17),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp.Int('input_unit',min_value=32,max_value=256,step=32))),\n        tf.keras.layers.Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)),\n        tf.keras.layers.Dense(1,activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu'))\n    ])\n\n\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                  optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate',min_value=0,max_value=0.5,step=0.0001)),\n                  metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:33.308706Z","iopub.execute_input":"2022-10-09T02:12:33.309101Z","iopub.status.idle":"2022-10-09T02:12:33.317652Z","shell.execute_reply.started":"2022-10-09T02:12:33.309064Z","shell.execute_reply":"2022-10-09T02:12:33.316638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def build_model(hp):\n#     model = Sequential()\n#     model.add(Embedding(input_dim=len(np.array(train_vectors.todense()[0])[0]), output_dim=32)),\n#     model.add(LSTM(hp.Int('input_unit',min_value=32,max_value=256,step=32),return_sequences=False))\n#   #  model.add(LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=256,step=32)))\n#     model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n#     model.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n#     model.compile(loss=MeanAbsoluteError(),\n#                   optimizer=Adam(1e-4),\n#                   metrics=['accuracy'])\n#     model.summary()\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:33.319107Z","iopub.execute_input":"2022-10-09T02:12:33.319876Z","iopub.status.idle":"2022-10-09T02:12:33.331603Z","shell.execute_reply.started":"2022-10-09T02:12:33.319839Z","shell.execute_reply":"2022-10-09T02:12:33.330707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    epochs_standard = 100\n    tuner = kt.Hyperband(\n        hypermodel=build_model_1,\n        objective=kt.Objective(name=\"val_accuracy\",direction=\"max\"),\n        max_epochs=epochs_standard,\n        factor=3,\n        hyperband_iterations=1,\n        overwrite=True\n    )\n    stop_early = EarlyStopping(monitor='val_accuracy', patience=5)\n\n    tuner.search(X,y, epochs=epochs_standard, validation_split=0.2, callbacks=[stop_early])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:12:33.333071Z","iopub.execute_input":"2022-10-09T02:12:33.333477Z","iopub.status.idle":"2022-10-09T03:34:09.782340Z","shell.execute_reply.started":"2022-10-09T02:12:33.333443Z","shell.execute_reply":"2022-10-09T03:34:09.781339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=3)[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:34:09.784071Z","iopub.execute_input":"2022-10-09T03:34:09.784459Z","iopub.status.idle":"2022-10-09T03:34:09.791700Z","shell.execute_reply.started":"2022-10-09T03:34:09.784409Z","shell.execute_reply":"2022-10-09T03:34:09.790596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nThe hyperparameter search is complete. The optimal drop out rate of layers for the optimizer\nis {best_hps.get('Dropout_rate')}. The optimal dense_activation is {best_hps.get('dense_activation')}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:34:09.792966Z","iopub.execute_input":"2022-10-09T03:34:09.793241Z","iopub.status.idle":"2022-10-09T03:34:09.806533Z","shell.execute_reply.started":"2022-10-09T03:34:09.793217Z","shell.execute_reply":"2022-10-09T03:34:09.804952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model with the optimal hyperparameters and train it on the data for x epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X,y, epochs=epochs_standard, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:34:09.808111Z","iopub.execute_input":"2022-10-09T03:34:09.808433Z","iopub.status.idle":"2022-10-09T03:37:37.746051Z","shell.execute_reply.started":"2022-10-09T03:34:09.808392Z","shell.execute_reply":"2022-10-09T03:37:37.745009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:37.748008Z","iopub.execute_input":"2022-10-09T03:37:37.748362Z","iopub.status.idle":"2022-10-09T03:37:37.756407Z","shell.execute_reply.started":"2022-10-09T03:37:37.748329Z","shell.execute_reply":"2022-10-09T03:37:37.753327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypermodel = tuner.hypermodel.build(best_hps)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:37.758271Z","iopub.execute_input":"2022-10-09T03:37:37.759139Z","iopub.status.idle":"2022-10-09T03:37:38.710647Z","shell.execute_reply.started":"2022-10-09T03:37:37.759103Z","shell.execute_reply":"2022-10-09T03:37:38.709609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain the model\nhypermodel.fit(X,y,epochs=best_epoch, validation_split=0.4)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:38.712478Z","iopub.execute_input":"2022-10-09T03:37:38.713404Z","iopub.status.idle":"2022-10-09T03:37:47.250976Z","shell.execute_reply.started":"2022-10-09T03:37:38.713363Z","shell.execute_reply":"2022-10-09T03:37:47.249995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:47.252192Z","iopub.execute_input":"2022-10-09T03:37:47.253156Z","iopub.status.idle":"2022-10-09T03:37:47.259357Z","shell.execute_reply.started":"2022-10-09T03:37:47.253119Z","shell.execute_reply":"2022-10-09T03:37:47.258310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:47.261833Z","iopub.execute_input":"2022-10-09T03:37:47.262092Z","iopub.status.idle":"2022-10-09T03:37:47.696204Z","shell.execute_reply.started":"2022-10-09T03:37:47.262068Z","shell.execute_reply":"2022-10-09T03:37:47.695023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here.","metadata":{}},{"cell_type":"markdown","source":"The above scores aren't terrible! It looks like our assumption will score roughly 0.65 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM / RNNs, the list is long!) - give any of them a shot!\n\nIn the meantime, let's do predictions on our training set and build a submission for the competition.","metadata":{}},{"cell_type":"code","source":"predict = model.predict(df_test_padded)\npredict","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:39:14.301679Z","iopub.execute_input":"2022-10-09T03:39:14.302390Z","iopub.status.idle":"2022-10-09T03:39:14.693593Z","shell.execute_reply.started":"2022-10-09T03:39:14.302347Z","shell.execute_reply":"2022-10-09T03:39:14.692638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = np.around(predict)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:40:18.334816Z","iopub.execute_input":"2022-10-09T03:40:18.335200Z","iopub.status.idle":"2022-10-09T03:40:18.340874Z","shell.execute_reply.started":"2022-10-09T03:40:18.335169Z","shell.execute_reply":"2022-10-09T03:40:18.339619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:37:49.137476Z","iopub.execute_input":"2022-10-09T03:37:49.138590Z","iopub.status.idle":"2022-10-09T03:37:49.151105Z","shell.execute_reply.started":"2022-10-09T03:37:49.138552Z","shell.execute_reply":"2022-10-09T03:37:49.150114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame({\n        \"id\": sample_submission[\"id\"],\n        \"target\":predict.flatten()\n    })","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:40:21.215558Z","iopub.execute_input":"2022-10-09T03:40:21.216263Z","iopub.status.idle":"2022-10-09T03:40:21.221336Z","shell.execute_reply.started":"2022-10-09T03:40:21.216227Z","shell.execute_reply":"2022-10-09T03:40:21.220368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:40:29.710187Z","iopub.execute_input":"2022-10-09T03:40:29.710904Z","iopub.status.idle":"2022-10-09T03:40:29.720256Z","shell.execute_reply.started":"2022-10-09T03:40:29.710867Z","shell.execute_reply":"2022-10-09T03:40:29.719066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:41:25.960971Z","iopub.execute_input":"2022-10-09T03:41:25.961350Z","iopub.status.idle":"2022-10-09T03:41:25.971508Z","shell.execute_reply.started":"2022-10-09T03:41:25.961318Z","shell.execute_reply":"2022-10-09T03:41:25.970440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv(\"submision.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:41:29.830859Z","iopub.execute_input":"2022-10-09T03:41:29.831231Z","iopub.status.idle":"2022-10-09T03:41:29.844464Z","shell.execute_reply.started":"2022-10-09T03:41:29.831201Z","shell.execute_reply":"2022-10-09T03:41:29.843364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, in the viewer, you can submit the above file to the competition! Good luck!","metadata":{}}]}